{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [WIP] SCIS 2026 \"セキュリティ用オープンLLMエージェントシステムのエンジニアリングガイド\" のケーススタディ実装\n",
        "\n",
        "本ノートブックは，上記論文の第4章「ケーススタディ」における実装を示すものです．ただし，論文中の記述に対して本ノートブック内の実装には以下の相違点があります．なお，これらはLLMエージェントの論理的な設計に影響を及ぼすものではありません．\n",
        "\n",
        "- Google Colaboratoryが使用するTesla T4 GPUは世代が古くSGLangが実質的に動作しないため，推論フレームワークにvLLMを使用しています．\n",
        "- Google Colaboratoryが使用するTesla T4 GPUは世代が古く小数精度としてFP8フォーマットをサポートしないため，KVキャッシュ量子化は使用していません．\n",
        "\n"
      ],
      "metadata": {
        "id": "G0_-J90aPnvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 初期セットアップ（必ず実行してください）"
      ],
      "metadata": {
        "id": "y0GQFMUVJf5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず，GPUが使用可能なランタイムであることを確認するために以下のセルを実行してください．"
      ],
      "metadata": {
        "id": "BCtqoR_hJm9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import HTML, display\n",
        "\n",
        "html_alertbox = HTML(\"\"\"\n",
        "<div class=\"alert\">\n",
        "  <p>このランタイムではGPUが使用できません．画面の上にある「ランタイム」→「ランタイムのタイプを変更」→「T4 GPU」を選択して，GPUランタイムに切り替えてください．</p>\n",
        "</div>\n",
        "\n",
        "<style>\n",
        ".alert {\n",
        "  padding: 20px;\n",
        "  background-color: #f44336;\n",
        "  color: white;\n",
        "  margin-bottom: 15px;\n",
        "}\n",
        "</style>\n",
        "\"\"\")\n",
        "\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  display(html_alertbox)\n",
        "  raise Exception(\"GPUが使用できません．GPUランタイムに切り替えてください．\")"
      ],
      "metadata": {
        "id": "Cqs07OLXKgl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に，セルの出力が長い場合に折り返されるように設定します．"
      ],
      "metadata": {
        "id": "70kocxKGk67Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable wrapping for the output cells\n",
        "from IPython.display import HTML, display\n",
        "from IPython.core.getipython import get_ipython\n",
        "def set_css(_):\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap !important;\n",
        "        word-wrap: break-word !important;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "u9wFqhNWk6Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に，以下のセルで必要なライブラリをインストールします．"
      ],
      "metadata": {
        "id": "btF1u65ENV7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/t0d4/SecurityLLMAgentDesignGuide-SCIS2026.git\n",
        "%cd SecurityLLMAgentDesignGuide-SCIS2026\n",
        "!uv pip install --system -r requirements.txt\n",
        "\n",
        "!uv pip install --system vllm==0.11.0 langchain langchain-deepseek pydantic"
      ],
      "metadata": {
        "id": "FFEJkWA7NZeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "OPENAI_BASE_URL=\"http://localhost:11434/v1\" \\\n",
        "    OPENAI_API_KEY=\"sk-dummy\" \\\n",
        "    python -m openai api chat.completions.create \\\n",
        "    -g user \"GPUに関する俳句を詠んでください．その後に短い解説をつけてください．\" \\\n",
        "    -m \"qwen3:8b\""
      ],
      "metadata": {
        "id": "8MgQ1njjwnMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM推論フレームワークの起動"
      ],
      "metadata": {
        "id": "-84wxXCZlGVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPTQによりInt4量子化された `Qwen3-4B-Thinking-2507` を動作させるサーバを起動します．初回実行時にはモデルをダウンロードする必要があるため時間がかかります．"
      ],
      "metadata": {
        "id": "_8VIArsMlPx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# SGLangは既にGoogle ColabのTesla T4 GPUをサポート対象外としており低速にしか動作しないため，ここでは使用しません．\n",
        "# nohup python -m sglang.launch_server \\\n",
        "#   --model-path JunHowie/Qwen3-4B-Thinking-2507-GPTQ-Int4 \\\n",
        "#   --mem-fraction-static 0.8 \\\n",
        "#   --context-length 32768 \\\n",
        "#   --reasoning-parser qwen3 \\\n",
        "#   --tool-call-parser qwen \\\n",
        "#   --kv-cache-dtype fp8_e4m3 &\n",
        "\n",
        "# Google Colab上では代わりにvLLMを使用します\n",
        "# For details of reasoning parser configuration, please visit https://github.com/vllm-project/vllm/issues/26239#issuecomment-3385327094\n",
        "nohup uv run vllm serve JunHowie/Qwen3-4B-Thinking-2507-GPTQ-Int4 \\\n",
        "  --served-model-name Qwen3-4B-Thinking-2507 \\\n",
        "  --gpu-memory-utilization 0.85 \\\n",
        "  --max-model-len 32768 \\\n",
        "  --reasoning-parser deepseek_r1 \\\n",
        "  --enable-auto-tool-choice \\\n",
        "  --tool-call-parser hermes \\\n",
        "  --host 127.0.0.1 --port 8000 > nohup.out 2>&1 &\n",
        "\n",
        "# vLLMの起動が完了するまで待機します (ログ出力はターミナルから `tail -f nohup.out` で確認できます)\n",
        "until curl -sf http://127.0.0.1:8000/health; do\n",
        "  sleep 5\n",
        "done\n",
        "echo \"vLLM startup completed.\""
      ],
      "metadata": {
        "id": "IXv9vFeBlMPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervisor-Worker型マルチエージェントシステムの実装"
      ],
      "metadata": {
        "id": "6Qy1DyrhfVBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず，vLLMのサーバをバックエンドとするクライアントを定義します．\n",
        "\n",
        "注) 以下では `ChatDeepSeek` を使用していますが，バックエンドのモデルはDeepSeekではなく上で述べた通りQwen3-4B-Thinking-2507です．これは以下の理由によります:\n",
        "\n",
        "- vLLMはOpenAI互換APIを提供するため `ChatOpenAI` からも接続可能だが，その場合[応答にReasoning途中の出力トークン (Reasoning Tokens) を含めないOpenAI APIの仕様](https://platform.openai.com/docs/guides/reasoning#reasoning-summaries)によりReasoning Tokensが取得できない\n",
        "- DeepSeek APIはOpenAI互換APIとして提供されており，かつReasoning Tokensをユーザへの応答に含める\n",
        "- したがって， `ChatDeepSeek` を使うことでvLLMに接続でき，かつReasoning Tokensを取得できる"
      ],
      "metadata": {
        "id": "nKjQrRCZm2S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_deepseek import ChatDeepSeek\n",
        "\n",
        "MODEL_NAME = \"Qwen3-4B-Thinking-2507\"\n",
        "\n",
        "llm = ChatDeepSeek(\n",
        "    model=MODEL_NAME,\n",
        "    name=MODEL_NAME,\n",
        "    api_base=\"http://127.0.0.1:8000/v1\",\n",
        "    api_key=\"sk-dummy\",\n",
        "    temperature=0.6,\n",
        "    top_p=0.95,\n",
        "    extra_body={\n",
        "        \"top_k\": 20,\n",
        "        \"separate_reasoning\": True\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "GQszBE-akIEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deobfuscatorの実装"
      ],
      "metadata": {
        "id": "B6wJ_1K_j63d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deobfuscatorに与えるツールを実装します．ここでは，base64ペイロードをデコードするためのツールとして `decode_base64_payload` を用意します．"
      ],
      "metadata": {
        "id": "j6A-lO-GqLD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "class DecodeBase64PayloadInput(BaseModel):\n",
        "    payload: str = Field(description=\"A payload to decode\")\n",
        "\n",
        "\n",
        "@tool(args_schema=DecodeBase64PayloadInput)\n",
        "def decode_base64_payload(payload: str) -> str:\n",
        "    \"\"\"Decode base64-encoded strings.\n",
        "    Runtime decoding of base64 data is done using the `base64.b64decode()` function.\n",
        "    If you encounter base64-encoded data, this tool will decode it for you.\n",
        "    When decoding fails, the error message will be shown to you.\n",
        "\n",
        "    Args:\n",
        "        payload (str): a string encoded in base64 format\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded content or error message with forensic details\n",
        "\n",
        "    Example success:\n",
        "        \"Hello, World!\" (when decoding \"SGVsbG8sIFdvcmxkIQ==\")\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import base64\n",
        "\n",
        "        decoded = base64.b64decode(payload.encode()).decode().replace(\"\\0\", \"\")\n",
        "    except Exception as e:\n",
        "        return (\n",
        "            f\"Failed to decode the base64 string. Error message is the following: {e}. \"\n",
        "            \"This could be due to incorrect payload, but also consider the possibility that this could be a random string and may not be a base64 string.\"\n",
        "        )\n",
        "    else:\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "XRoIbPKN5tux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deobfuscator本体をエージェントとして定義します．"
      ],
      "metadata": {
        "id": "cUUnN8_V6W8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_agent\n",
        "\n",
        "\n",
        "# Workerエージェントが最終出力を行う際に従うべきフォーマットを指定します．\n",
        "# https://docs.langchain.com/oss/python/langchain/structured-output\n",
        "class WorkerResponse(BaseModel):\n",
        "    \"\"\"Standard format for a worker agent response\"\"\"\n",
        "    detailed_report: str = Field(description=\"detailed, markdown-style report containing all details discovered during the task\")\n",
        "    short_summary: str = Field(description=\"minimal-length focused summary articulating the essential findings\")\n",
        "\n",
        "\n",
        "deobfuscator = create_agent(\n",
        "    model=llm,\n",
        "    tools=[decode_base64_payload],\n",
        "    system_prompt=\"\",\n",
        "    response_format=WorkerResponse,\n",
        ")"
      ],
      "metadata": {
        "id": "1Lu71v-EhckH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}